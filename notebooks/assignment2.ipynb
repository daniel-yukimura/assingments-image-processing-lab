{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Classification\n",
    "\n",
    "This notebook contains examples, plus the exercises for the second assignment, which should be submitted as instructed in the assignments [page](https://daniel-yukimura.github.io/assingments-image-processing-lab/).<br>\n",
    "\n",
    "In this assignment we'll apply some of the methods we have seen in class for designing and training neural networks.\n",
    "\n",
    "### Example: MNIST classifier.\n",
    "\n",
    "MNIST, is a famous dataset containing 70,000 grayscale hand-written digits (0-9) and their associated values. PyTorch **torchvision.dataset** gives us access to a few datasets including this one. The dataset is divided as 60,000 training images and 10,000 for the test set.<br>\n",
    "Our goal is to construct a neural network classifier that can read the digits in the image. This example should be seen as a starting point for the assignment exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#hyperparameter:\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "classes = ('0','1','2','3','4','5','6','7','8','9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3     1     5     4     6     5     4     8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code will plot some examples\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0:8]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.1\n",
    "#creating model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.7229\n",
      "Epoch [1/5], Step [200/600], Loss: 0.4161\n",
      "Epoch [1/5], Step [300/600], Loss: 0.3334\n",
      "Epoch [1/5], Step [400/600], Loss: 0.3137\n",
      "Epoch [1/5], Step [500/600], Loss: 0.3406\n",
      "Epoch [1/5], Step [600/600], Loss: 0.3054\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1413\n",
      "Epoch [2/5], Step [200/600], Loss: 0.3348\n",
      "Epoch [2/5], Step [300/600], Loss: 0.4180\n",
      "Epoch [2/5], Step [400/600], Loss: 0.2555\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1619\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1447\n",
      "Epoch [3/5], Step [100/600], Loss: 0.2768\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1887\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1791\n",
      "Epoch [3/5], Step [400/600], Loss: 0.1806\n",
      "Epoch [3/5], Step [500/600], Loss: 0.2878\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1790\n",
      "Epoch [4/5], Step [100/600], Loss: 0.1534\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1794\n",
      "Epoch [4/5], Step [300/600], Loss: 0.2331\n",
      "Epoch [4/5], Step [400/600], Loss: 0.2594\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1295\n",
      "Epoch [4/5], Step [600/600], Loss: 0.2837\n",
      "Epoch [5/5], Step [100/600], Loss: 0.1240\n",
      "Epoch [5/5], Step [200/600], Loss: 0.1261\n",
      "Epoch [5/5], Step [300/600], Loss: 0.1663\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0848\n",
      "Epoch [5/5], Step [500/600], Loss: 0.1889\n",
      "Epoch [5/5], Step [600/600], Loss: 0.2972\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96.16 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        labels = labels\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Improving the MNIST classifier\n",
    "\n",
    "In this exercise we want to improve our classifier's performance and apply some of the techniques presented in class.<br>\n",
    "You should follow each of the following points, and comment on the improvements or issues caused by the changes. After completing the steps you should explain which setting works best. In this part you can apply your own modifications, and feel free to look up online for suggestions.\n",
    "* Create a deeper network, test performance using 2 and 4 layers (you can try something like $500\\times250$ and $500\\times250\\times100\\times25$).\n",
    "* Change the optimization method to **torch.optim.Adam**. Test it with both the one hidden layer model and multiple layers. You'll probably need to lower your **learning_rate**, observe how this is relevant when the number of layers is higher.\n",
    "* Add dropout layers and observe how this affects performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: CIFAR dataset\n",
    "\n",
    "This time we'll use another dataset, the CIFAR10, also provided by the torchvision library. It contains images referent to classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "* Load the images as shown before, and use the fully-connected model you think is appropriated (the one you chose in exercise 1).\n",
    "* Next, create a convolutional network with the following architecture:\n",
    "  - Convolution with 5 by 5 filters, 16 feature maps + Tanh nonlinearity.\n",
    "  - 2 by 2 max pooling.\n",
    "  - Convolution with 5 by 5 filters, 128 feature maps + Tanh nonlinearity.\n",
    "  - 2 by 2 max pooling.\n",
    "  - Flatten to vector.\n",
    "  - Linear layer with 64 hidden units + Tanh nonlinearity.\n",
    "  - Linear layer to 10 output units.\n",
    "* Train it for 20 epochs on the CIFAR-10 training set and test performance.\n",
    "* Use different techniques to visualize performance. \n",
    "  - Create a function that shows examples from test set with true and predicted label\n",
    "  - Plot the confusion matrix of your results.\n",
    "  - Plot the images with worst performance, i.e. where it the network missclassified and the confidence of the network was low, i.e. the output probability was low.\n",
    "  - (extra)  Add code to plot out the network weights as images, to visualize how the weights respond to an image.\n",
    "  \n",
    "  \n",
    "You can check [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) for some help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Dogs vs Cats\n",
    "\n",
    "Download the dataset on [http://files.fast.ai/data/dogscats.zip](http://files.fast.ai/data/dogscats.zip), there are 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that you have to try to label. This time you will need to figure out a way to read and tranform the images into tensors.\n",
    "\n",
    "* Train a CNN classifier on this dataset, you can start from the model suggested on exercise 2.\n",
    "* Try to improve your result adding regularization (dropout for example).\n",
    "* Repeat the visualization part from exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:image-processing-lab]",
   "language": "python",
   "name": "conda-env-image-processing-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
